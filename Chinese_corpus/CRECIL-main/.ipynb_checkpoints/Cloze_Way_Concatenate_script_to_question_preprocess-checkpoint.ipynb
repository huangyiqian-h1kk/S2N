{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33a8b782",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "import zhon\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "import pandas as pd\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from transformers import set_seed\n",
    "from transformers import BertTokenizerFast\n",
    "from transformers import TFAutoModelForMaskedLM\n",
    "from transformers import BertForPreTraining, BertForMaskedLM, BertConfig\n",
    "import transformers\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset, Dataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.optim import AdamW\n",
    "import argparse, torch, datasets, ast, operator, gc, os\n",
    "import torch.nn as nn\n",
    "import json\n",
    "import csv\n",
    "from math import ceil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c637466",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(21128, 1024, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 1024)\n",
       "      (token_type_embeddings): Embedding(2, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=1024, out_features=21128, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='search for best template according to dev set')\n",
    "parser.add_argument('--max_len', default=512, type=int, help=\"max sequence length\")\n",
    "parser.add_argument('--model', default='/home/lr/h1k/KNN-BERT-main/csi_sequetial_fine_tuning//MLM_task_tuned_models_for _datasets_PPL/MLM_tuned_roberta_large_JY_te+JY_Tr+CSI_tr/', type=str, help=\"pretrained model\")\n",
    "#parser.add_argument('--model', default='./chinese_roberta_wwm_large_ext_pytorch/')\n",
    "parser.add_argument('--result_output_dir', default='/home/lr/h1k/KNN-BERT-main/csi_sequetial_fine_tuning/Tune_on_CSI/csi_output/ppl_results/', type=str, help=\"output directory\")\n",
    "parser.add_argument('--tokenizer', default='/home/lr/h1k/KNN-BERT-main/csi_sequetial_fine_tuning/MLM_task_tuned_models_for _datasets_PPL/MLM_tuned_roberta_large_JY_te+JY_Tr+CSI_tr/', type=str, help=\"tokenizer\")\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "# 加载模型和分词器\n",
    "model = BertForMaskedLM.from_pretrained(args.model)\n",
    "tokenizer = BertTokenizerFast.from_pretrained(f'{args.tokenizer}')\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "#if torch.cuda.device_count() > 1:\n",
    "#model = torch.nn.DataParallel(model)\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "43337d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"My_home.json\") as f:\n",
    "    data=json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cb28a800",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['episodes'][0]['episode_id'].strip(\"ep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4987257f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['episodes'])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f05a254b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data['episodes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f48c326c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data['episodes'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5398a4ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['episode_id', 'scenes'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['episodes'][0].keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6616f3c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data['episodes'][0]['scenes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "18c4d944",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'utterance_id': 'ep1_sc1_u1',\n",
       " 'speakers': '圆圆',\n",
       " 'transcript': '爸，动画片儿哪频道啊？',\n",
       " 'tokens': ['爸', '，', '动画片儿', '哪', '频道', '啊', '？', ''],\n",
       " 'character_entities': [['爸', 'jzg', 0]]}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['episodes'][0]['scenes'][0]['utterances'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ceb42a95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data['episodes'][0]['scenes'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "13b8b82b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['scene_id', 'utterances'])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['episodes'][0]['scenes'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8073f3a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['utterance_id', 'speakers', 'transcript', 'tokens', 'character_entities'])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['episodes'][0]['scenes'][0]['utterances'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0cd09dec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'爸，动画片儿哪频道啊？'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['episodes'][0]['scenes'][0]['utterances'][0]['transcript']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f062ca8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'圆圆'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['episodes'][0]['scenes'][0]['utterances'][0]['speakers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "19557b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(path_from,path_to, vocab_file,vocab_bond):\n",
    "    \n",
    "    with open(path_from) as f:\n",
    "        dj=json.load(f)\n",
    "    \n",
    "    vf=open(vocab_file) \n",
    "    vocab= vf.readlines()\n",
    "    vocab=[i.strip('\\n') for i in vocab][vocab_bond[0]:vocab_bond[1]]\n",
    "        \n",
    "    name=path_from.split('/')[-1].strip('.csv')\n",
    "    data= context_window(dj['episodes'][:10], vocab,name)\n",
    "    \n",
    "    with open(path_to, 'w') as f:\n",
    "        json.dump(data, f, ensure_ascii=False,indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "bbc2a311",
   "metadata": {},
   "outputs": [],
   "source": [
    "def examine_length(index_top, index_bottom, scene):\n",
    "    ref=''\n",
    "    for i in range(index_top, index_bottom):\n",
    "        ref+=scene[i]['speakers']+scene[i]['transcript']\n",
    "        window_num = ceil(len(ref)/512) \n",
    "    \n",
    "    return window_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "48f34493",
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_window(epis, vocab, name):\n",
    "    data_dict=dict()\n",
    "    data_dict[\"data\"]=list()\n",
    "    data_dict[\"version\"]=\"v1.0\",\n",
    "    \n",
    "    i_scene=0\n",
    "    for epi in epis:\n",
    "        scenes=epi[\"scenes\"]\n",
    "        for scene in scenes: \n",
    "            scene=scene['utterances']\n",
    "            \n",
    "            window_num = examine_length(0,len(scene),scene) \n",
    "            \n",
    "            if window_num==1:\n",
    "                utterances=[u['transcript'] for u in scene]\n",
    "                speakers=[u['speakers'] for u in scene]\n",
    "                masked_sets=get_masked_sets(speakers)\n",
    "\n",
    "                for m_speakers in masked_sets:\n",
    "                    paragraphs=lines_to_question(speakers,m_speakers, utterances, vocab, i_scene, name)\n",
    "                    data_dict[\"data\"].append({\"paragraphs\":paragraphs})\n",
    "                i_scene +=1\n",
    "                    \n",
    "            else:\n",
    "                norm_window_size=len(scene)//window_num\n",
    "                pointer=0\n",
    "                print(len(scene),window_num,norm_window_size)\n",
    "                while pointer < len(scene)-5:\n",
    "                    window_size=random.randint(norm_window_size-3,norm_window_size)\n",
    "                    if not examine_length(pointer, pointer+window_size,scene)==1:\n",
    "                        continue\n",
    "                    print(window_size)\n",
    "                    utterances=[u['transcript'] for u in scene[pointer:pointer+window_size]]\n",
    "                    speakers=[u['speakers'] for u in scene[pointer:pointer+window_size]]\n",
    "                    \n",
    "                    masked_sets=get_masked_sets(speakers)\n",
    "    \n",
    "                    for m_speakers in masked_sets:\n",
    "                        paragraphs=lines_to_question(speakers,m_speakers, utterances, vocab, i_scene, name)\n",
    "                        data_dict[\"data\"].append({\"paragraphs\":paragraphs})\n",
    "                    i_scene +=1\n",
    "                    pointer += window_size\n",
    "                    \n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "16208e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def noised_speaker(NAME):\n",
    "    name=NAME\n",
    "    while name==NAME or set(name)==1:\n",
    "        if random.random()<0.4:\n",
    "            mask=np.random.random(size = (len(name)))<0.5\n",
    "            name=''.join([name[i] if mask[i]==True else '□' for i in range(len(name))])\n",
    "        else:\n",
    "            name=''\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "aca44439",
   "metadata": {},
   "outputs": [],
   "source": [
    "def structure_utter(SPEAKER,UTTER,T):\n",
    "    R1=(random.random()<T)*(SPEAKER!=\"\")\n",
    "    R2=random.random()<T\n",
    "    return R1*'：' + R2*'“' + UTTER + R2*'”'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "123b7392",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_masked_sets(s):\n",
    "\n",
    "    #mask speakers\n",
    "    speakers=s.copy()\n",
    "    speaker_pointer=dict()\n",
    "    for i in range(len(speakers)):\n",
    "        if speakers[i] in speaker_pointer.keys():\n",
    "            speaker_pointer[speakers[i]].append(i)\n",
    "        else:\n",
    "            speaker_pointer[speakers[i]]=list()\n",
    "            speaker_pointer[speakers[i]].append(i)\n",
    "    \n",
    "    random_chance=1\n",
    "    unmask_pos=list()\n",
    "    while random_chance>=0:\n",
    "        res=[random.choice(l) for l in speaker_pointer.values()]\n",
    "        if not res in unmask_pos:\n",
    "            unmask_pos.append(res)\n",
    "        else:\n",
    "            random_chance-=1\n",
    "            \n",
    "    masked_speakers=list()        \n",
    "    for pos in unmask_pos:\n",
    "        this_list=[noised_speaker(speakers[i]) if not i in pos else speakers[i] for i in range(len(speakers))]\n",
    "        masked_speakers.append(this_list)\n",
    "        \n",
    "    return masked_speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f0482e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lines_to_question(s,ms,u,v, index, name):\n",
    "    para=[{'id':name+\"_\"+str(index),'context':'','qas':[]}]\n",
    "    context, answer= concat_with_noise(set(s),ms,u,v)\n",
    "    para[-1][\"context\"]=context\n",
    "    \n",
    "    count=1\n",
    "    qas=list()\n",
    "\n",
    "    for i in range(len(u)):    \n",
    "        question={'question':u[i],'id':name+\"_\"+str(index)+\"_\"+str(count),'answers':[{\"answer_start\":answer[s[i]],\"text\":s[i]}]}\n",
    "        count+=1\n",
    "        qas.append(question)\n",
    "        \n",
    "    para[-1][\"qas\"]=qas\n",
    "    return para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e026a173",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_with_noise(speaker_set, ms, u, v, cloze_range=3, threshold1=0.5):\n",
    "    mspeakers=ms.copy()\n",
    "\n",
    "    cont=cloze(u, mspeakers, cloze_range, threshold1)\n",
    "    speaker_answer=dict()\n",
    "    \n",
    "    for speaker in speaker_set:\n",
    "        speaker_answer[speaker]=cont.find(speaker)\n",
    " \n",
    "\n",
    "    return cont, speaker_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79b337d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cloze(utter, m_speaker, cloze_range, threshold1):\n",
    "    sequence=''\n",
    "    for i in range(len(m_speaker)):\n",
    "        ranint1=random.randint(0,cloze_range)\n",
    "        ranint2=random.randint(0,cloze_range)\n",
    "        random_threshold1= threshold1\n",
    "        \n",
    "        sequence+=m_speaker[i]+(m_speaker[i]!=\"\")*ranint1*tokenizer.mask_token + structure_utter(m_speaker[i],utter[i]+ tokenizer.mask_token*ranint2, random_threshold1) + tokenizer.sep_token\n",
    "\n",
    "    input = tokenizer.encode(sequence, return_tensors=\"pt\")\n",
    "    mask_token_index = torch.where(input == tokenizer.mask_token_id)[1]\n",
    "    token_logits = model(input.cuda()).logits\n",
    "    mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "    top_1_tokens = torch.topk(mask_token_logits, 1, dim=1).indices.squeeze(1)\n",
    "    \n",
    "    for i in range(len(mask_token_index)):\n",
    "        sequence=sequence.replace(tokenizer.mask_token, tokenizer.decode(top_1_tokens[i]),1)\n",
    "    sequence=sequence.replace(tokenizer.sep_token,'')\n",
    "    \n",
    "    return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9a23c6ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 3 13\n",
      "12\n",
      "['爸，动画片儿哪频道啊？', '看哪门子动画片呀----看连续剧', '小张你这菜可咸点儿啊！', '二哥，你霸着鸡腿儿你一人吃你能不咸嘛你，人家一礼拜可才见一回晕腥！唉唉唉，动物世界动物世界……', '哎哎哎，咱爸今儿是怎么回事儿？从单位一回来就打蔫儿，饭都不吃就楼底下溜达去了这大冷的天儿……', '可能是添什么心事了，唉，是不是今儿跟单位巧遇哪位中午丧偶的女同志……', '说话不着调……', '咱妈去世两年多了，咱爸论条件，论身体，要为这烦恼，那还不要多正常有多正常？', '咱爸这岁数……不至于……', '爱情不分老少，人生没有单行道。', '嘿嘿嘿，这孩子哪儿学来这么些乱七八糟的……', '还不都是你们招的！咱爸上了一天班，累了，躺会儿，你们就非说他老人家惦记给咱们找后妈，这是那儿跟那儿啊！']\n",
      "['爸，动画片儿哪频道啊？', '看哪门子动画片呀----看连续剧', '小张你这菜可咸点儿啊！', '二哥，你霸着鸡腿儿你一人吃你能不咸嘛你，人家一礼拜可才见一回晕腥！唉唉唉，动物世界动物世界……', '哎哎哎，咱爸今儿是怎么回事儿？从单位一回来就打蔫儿，饭都不吃就楼底下溜达去了这大冷的天儿……', '可能是添什么心事了，唉，是不是今儿跟单位巧遇哪位中午丧偶的女同志……', '说话不着调……', '咱妈去世两年多了，咱爸论条件，论身体，要为这烦恼，那还不要多正常有多正常？', '咱爸这岁数……不至于……', '爱情不分老少，人生没有单行道。', '嘿嘿嘿，这孩子哪儿学来这么些乱七八糟的……', '还不都是你们招的！咱爸上了一天班，累了，躺会儿，你们就非说他老人家惦记给咱们找后妈，这是那儿跟那儿啊！']\n",
      "['爸，动画片儿哪频道啊？', '看哪门子动画片呀----看连续剧', '小张你这菜可咸点儿啊！', '二哥，你霸着鸡腿儿你一人吃你能不咸嘛你，人家一礼拜可才见一回晕腥！唉唉唉，动物世界动物世界……', '哎哎哎，咱爸今儿是怎么回事儿？从单位一回来就打蔫儿，饭都不吃就楼底下溜达去了这大冷的天儿……', '可能是添什么心事了，唉，是不是今儿跟单位巧遇哪位中午丧偶的女同志……', '说话不着调……', '咱妈去世两年多了，咱爸论条件，论身体，要为这烦恼，那还不要多正常有多正常？', '咱爸这岁数……不至于……', '爱情不分老少，人生没有单行道。', '嘿嘿嘿，这孩子哪儿学来这么些乱七八糟的……', '还不都是你们招的！咱爸上了一天班，累了，躺会儿，你们就非说他老人家惦记给咱们找后妈，这是那儿跟那儿啊！']\n",
      "['爸，动画片儿哪频道啊？', '看哪门子动画片呀----看连续剧', '小张你这菜可咸点儿啊！', '二哥，你霸着鸡腿儿你一人吃你能不咸嘛你，人家一礼拜可才见一回晕腥！唉唉唉，动物世界动物世界……', '哎哎哎，咱爸今儿是怎么回事儿？从单位一回来就打蔫儿，饭都不吃就楼底下溜达去了这大冷的天儿……', '可能是添什么心事了，唉，是不是今儿跟单位巧遇哪位中午丧偶的女同志……', '说话不着调……', '咱妈去世两年多了，咱爸论条件，论身体，要为这烦恼，那还不要多正常有多正常？', '咱爸这岁数……不至于……', '爱情不分老少，人生没有单行道。', '嘿嘿嘿，这孩子哪儿学来这么些乱七八糟的……', '还不都是你们招的！咱爸上了一天班，累了，躺会儿，你们就非说他老人家惦记给咱们找后妈，这是那儿跟那儿啊！']\n",
      "['爸，动画片儿哪频道啊？', '看哪门子动画片呀----看连续剧', '小张你这菜可咸点儿啊！', '二哥，你霸着鸡腿儿你一人吃你能不咸嘛你，人家一礼拜可才见一回晕腥！唉唉唉，动物世界动物世界……', '哎哎哎，咱爸今儿是怎么回事儿？从单位一回来就打蔫儿，饭都不吃就楼底下溜达去了这大冷的天儿……', '可能是添什么心事了，唉，是不是今儿跟单位巧遇哪位中午丧偶的女同志……', '说话不着调……', '咱妈去世两年多了，咱爸论条件，论身体，要为这烦恼，那还不要多正常有多正常？', '咱爸这岁数……不至于……', '爱情不分老少，人生没有单行道。', '嘿嘿嘿，这孩子哪儿学来这么些乱七八糟的……', '还不都是你们招的！咱爸上了一天班，累了，躺会儿，你们就非说他老人家惦记给咱们找后妈，这是那儿跟那儿啊！']\n",
      "['爸，动画片儿哪频道啊？', '看哪门子动画片呀----看连续剧', '小张你这菜可咸点儿啊！', '二哥，你霸着鸡腿儿你一人吃你能不咸嘛你，人家一礼拜可才见一回晕腥！唉唉唉，动物世界动物世界……', '哎哎哎，咱爸今儿是怎么回事儿？从单位一回来就打蔫儿，饭都不吃就楼底下溜达去了这大冷的天儿……', '可能是添什么心事了，唉，是不是今儿跟单位巧遇哪位中午丧偶的女同志……', '说话不着调……', '咱妈去世两年多了，咱爸论条件，论身体，要为这烦恼，那还不要多正常有多正常？', '咱爸这岁数……不至于……', '爱情不分老少，人生没有单行道。', '嘿嘿嘿，这孩子哪儿学来这么些乱七八糟的……', '还不都是你们招的！咱爸上了一天班，累了，躺会儿，你们就非说他老人家惦记给咱们找后妈，这是那儿跟那儿啊！']\n",
      "['爸，动画片儿哪频道啊？', '看哪门子动画片呀----看连续剧', '小张你这菜可咸点儿啊！', '二哥，你霸着鸡腿儿你一人吃你能不咸嘛你，人家一礼拜可才见一回晕腥！唉唉唉，动物世界动物世界……', '哎哎哎，咱爸今儿是怎么回事儿？从单位一回来就打蔫儿，饭都不吃就楼底下溜达去了这大冷的天儿……', '可能是添什么心事了，唉，是不是今儿跟单位巧遇哪位中午丧偶的女同志……', '说话不着调……', '咱妈去世两年多了，咱爸论条件，论身体，要为这烦恼，那还不要多正常有多正常？', '咱爸这岁数……不至于……', '爱情不分老少，人生没有单行道。', '嘿嘿嘿，这孩子哪儿学来这么些乱七八糟的……', '还不都是你们招的！咱爸上了一天班，累了，躺会儿，你们就非说他老人家惦记给咱们找后妈，这是那儿跟那儿啊！']\n",
      "['爸，动画片儿哪频道啊？', '看哪门子动画片呀----看连续剧', '小张你这菜可咸点儿啊！', '二哥，你霸着鸡腿儿你一人吃你能不咸嘛你，人家一礼拜可才见一回晕腥！唉唉唉，动物世界动物世界……', '哎哎哎，咱爸今儿是怎么回事儿？从单位一回来就打蔫儿，饭都不吃就楼底下溜达去了这大冷的天儿……', '可能是添什么心事了，唉，是不是今儿跟单位巧遇哪位中午丧偶的女同志……', '说话不着调……', '咱妈去世两年多了，咱爸论条件，论身体，要为这烦恼，那还不要多正常有多正常？', '咱爸这岁数……不至于……', '爱情不分老少，人生没有单行道。', '嘿嘿嘿，这孩子哪儿学来这么些乱七八糟的……', '还不都是你们招的！咱爸上了一天班，累了，躺会儿，你们就非说他老人家惦记给咱们找后妈，这是那儿跟那儿啊！']\n",
      "10\n",
      "['姑姑，你说大人为什么都这么愿意上班啊？', '啊也不是所有的大人都愿意上班--比如你二叔，在家闲了三年他都不着急……', '谁闲着了？我挂着两个单位，兼着三个公司的副总经理，我比谁不忙呀！', '啊呸！你忙！咱爸他们单位，已经好几次想把爸的办公桌从局长室请出去啦！', '可你爸也是，反正也退了，挨家享两天轻福不好么！何必每星期还大老远的跑到局里去“顾问”两次--害得人家车接车送的，给国家省点儿气油好不好？', '老同志嘛，工作需要嘛！', '你算了吧！你爸呀，纯属老糊涂了！', '还不光是老……那是他们别人，咱爸可是越老越明白！', '根本不可能……那是绝对没错儿！我就同意二哥这观点！', '我就反对你们这样……谁反对你们我就反对谁！']\n",
      "['姑姑，你说大人为什么都这么愿意上班啊？', '啊也不是所有的大人都愿意上班--比如你二叔，在家闲了三年他都不着急……', '谁闲着了？我挂着两个单位，兼着三个公司的副总经理，我比谁不忙呀！', '啊呸！你忙！咱爸他们单位，已经好几次想把爸的办公桌从局长室请出去啦！', '可你爸也是，反正也退了，挨家享两天轻福不好么！何必每星期还大老远的跑到局里去“顾问”两次--害得人家车接车送的，给国家省点儿气油好不好？', '老同志嘛，工作需要嘛！', '你算了吧！你爸呀，纯属老糊涂了！', '还不光是老……那是他们别人，咱爸可是越老越明白！', '根本不可能……那是绝对没错儿！我就同意二哥这观点！', '我就反对你们这样……谁反对你们我就反对谁！']\n",
      "['姑姑，你说大人为什么都这么愿意上班啊？', '啊也不是所有的大人都愿意上班--比如你二叔，在家闲了三年他都不着急……', '谁闲着了？我挂着两个单位，兼着三个公司的副总经理，我比谁不忙呀！', '啊呸！你忙！咱爸他们单位，已经好几次想把爸的办公桌从局长室请出去啦！', '可你爸也是，反正也退了，挨家享两天轻福不好么！何必每星期还大老远的跑到局里去“顾问”两次--害得人家车接车送的，给国家省点儿气油好不好？', '老同志嘛，工作需要嘛！', '你算了吧！你爸呀，纯属老糊涂了！', '还不光是老……那是他们别人，咱爸可是越老越明白！', '根本不可能……那是绝对没错儿！我就同意二哥这观点！', '我就反对你们这样……谁反对你们我就反对谁！']\n",
      "['姑姑，你说大人为什么都这么愿意上班啊？', '啊也不是所有的大人都愿意上班--比如你二叔，在家闲了三年他都不着急……', '谁闲着了？我挂着两个单位，兼着三个公司的副总经理，我比谁不忙呀！', '啊呸！你忙！咱爸他们单位，已经好几次想把爸的办公桌从局长室请出去啦！', '可你爸也是，反正也退了，挨家享两天轻福不好么！何必每星期还大老远的跑到局里去“顾问”两次--害得人家车接车送的，给国家省点儿气油好不好？', '老同志嘛，工作需要嘛！', '你算了吧！你爸呀，纯属老糊涂了！', '还不光是老……那是他们别人，咱爸可是越老越明白！', '根本不可能……那是绝对没错儿！我就同意二哥这观点！', '我就反对你们这样……谁反对你们我就反对谁！']\n",
      "['姑姑，你说大人为什么都这么愿意上班啊？', '啊也不是所有的大人都愿意上班--比如你二叔，在家闲了三年他都不着急……', '谁闲着了？我挂着两个单位，兼着三个公司的副总经理，我比谁不忙呀！', '啊呸！你忙！咱爸他们单位，已经好几次想把爸的办公桌从局长室请出去啦！', '可你爸也是，反正也退了，挨家享两天轻福不好么！何必每星期还大老远的跑到局里去“顾问”两次--害得人家车接车送的，给国家省点儿气油好不好？', '老同志嘛，工作需要嘛！', '你算了吧！你爸呀，纯属老糊涂了！', '还不光是老……那是他们别人，咱爸可是越老越明白！', '根本不可能……那是绝对没错儿！我就同意二哥这观点！', '我就反对你们这样……谁反对你们我就反对谁！']\n",
      "['姑姑，你说大人为什么都这么愿意上班啊？', '啊也不是所有的大人都愿意上班--比如你二叔，在家闲了三年他都不着急……', '谁闲着了？我挂着两个单位，兼着三个公司的副总经理，我比谁不忙呀！', '啊呸！你忙！咱爸他们单位，已经好几次想把爸的办公桌从局长室请出去啦！', '可你爸也是，反正也退了，挨家享两天轻福不好么！何必每星期还大老远的跑到局里去“顾问”两次--害得人家车接车送的，给国家省点儿气油好不好？', '老同志嘛，工作需要嘛！', '你算了吧！你爸呀，纯属老糊涂了！', '还不光是老……那是他们别人，咱爸可是越老越明白！', '根本不可能……那是绝对没错儿！我就同意二哥这观点！', '我就反对你们这样……谁反对你们我就反对谁！']\n",
      "['姑姑，你说大人为什么都这么愿意上班啊？', '啊也不是所有的大人都愿意上班--比如你二叔，在家闲了三年他都不着急……', '谁闲着了？我挂着两个单位，兼着三个公司的副总经理，我比谁不忙呀！', '啊呸！你忙！咱爸他们单位，已经好几次想把爸的办公桌从局长室请出去啦！', '可你爸也是，反正也退了，挨家享两天轻福不好么！何必每星期还大老远的跑到局里去“顾问”两次--害得人家车接车送的，给国家省点儿气油好不好？', '老同志嘛，工作需要嘛！', '你算了吧！你爸呀，纯属老糊涂了！', '还不光是老……那是他们别人，咱爸可是越老越明白！', '根本不可能……那是绝对没错儿！我就同意二哥这观点！', '我就反对你们这样……谁反对你们我就反对谁！']\n",
      "['姑姑，你说大人为什么都这么愿意上班啊？', '啊也不是所有的大人都愿意上班--比如你二叔，在家闲了三年他都不着急……', '谁闲着了？我挂着两个单位，兼着三个公司的副总经理，我比谁不忙呀！', '啊呸！你忙！咱爸他们单位，已经好几次想把爸的办公桌从局长室请出去啦！', '可你爸也是，反正也退了，挨家享两天轻福不好么！何必每星期还大老远的跑到局里去“顾问”两次--害得人家车接车送的，给国家省点儿气油好不好？', '老同志嘛，工作需要嘛！', '你算了吧！你爸呀，纯属老糊涂了！', '还不光是老……那是他们别人，咱爸可是越老越明白！', '根本不可能……那是绝对没错儿！我就同意二哥这观点！', '我就反对你们这样……谁反对你们我就反对谁！']\n",
      "['姑姑，你说大人为什么都这么愿意上班啊？', '啊也不是所有的大人都愿意上班--比如你二叔，在家闲了三年他都不着急……', '谁闲着了？我挂着两个单位，兼着三个公司的副总经理，我比谁不忙呀！', '啊呸！你忙！咱爸他们单位，已经好几次想把爸的办公桌从局长室请出去啦！', '可你爸也是，反正也退了，挨家享两天轻福不好么！何必每星期还大老远的跑到局里去“顾问”两次--害得人家车接车送的，给国家省点儿气油好不好？', '老同志嘛，工作需要嘛！', '你算了吧！你爸呀，纯属老糊涂了！', '还不光是老……那是他们别人，咱爸可是越老越明白！', '根本不可能……那是绝对没错儿！我就同意二哥这观点！', '我就反对你们这样……谁反对你们我就反对谁！']\n",
      "['姑姑，你说大人为什么都这么愿意上班啊？', '啊也不是所有的大人都愿意上班--比如你二叔，在家闲了三年他都不着急……', '谁闲着了？我挂着两个单位，兼着三个公司的副总经理，我比谁不忙呀！', '啊呸！你忙！咱爸他们单位，已经好几次想把爸的办公桌从局长室请出去啦！', '可你爸也是，反正也退了，挨家享两天轻福不好么！何必每星期还大老远的跑到局里去“顾问”两次--害得人家车接车送的，给国家省点儿气油好不好？', '老同志嘛，工作需要嘛！', '你算了吧！你爸呀，纯属老糊涂了！', '还不光是老……那是他们别人，咱爸可是越老越明白！', '根本不可能……那是绝对没错儿！我就同意二哥这观点！', '我就反对你们这样……谁反对你们我就反对谁！']\n",
      "['姑姑，你说大人为什么都这么愿意上班啊？', '啊也不是所有的大人都愿意上班--比如你二叔，在家闲了三年他都不着急……', '谁闲着了？我挂着两个单位，兼着三个公司的副总经理，我比谁不忙呀！', '啊呸！你忙！咱爸他们单位，已经好几次想把爸的办公桌从局长室请出去啦！', '可你爸也是，反正也退了，挨家享两天轻福不好么！何必每星期还大老远的跑到局里去“顾问”两次--害得人家车接车送的，给国家省点儿气油好不好？', '老同志嘛，工作需要嘛！', '你算了吧！你爸呀，纯属老糊涂了！', '还不光是老……那是他们别人，咱爸可是越老越明白！', '根本不可能……那是绝对没错儿！我就同意二哥这观点！', '我就反对你们这样……谁反对你们我就反对谁！']\n",
      "['姑姑，你说大人为什么都这么愿意上班啊？', '啊也不是所有的大人都愿意上班--比如你二叔，在家闲了三年他都不着急……', '谁闲着了？我挂着两个单位，兼着三个公司的副总经理，我比谁不忙呀！', '啊呸！你忙！咱爸他们单位，已经好几次想把爸的办公桌从局长室请出去啦！', '可你爸也是，反正也退了，挨家享两天轻福不好么！何必每星期还大老远的跑到局里去“顾问”两次--害得人家车接车送的，给国家省点儿气油好不好？', '老同志嘛，工作需要嘛！', '你算了吧！你爸呀，纯属老糊涂了！', '还不光是老……那是他们别人，咱爸可是越老越明白！', '根本不可能……那是绝对没错儿！我就同意二哥这观点！', '我就反对你们这样……谁反对你们我就反对谁！']\n",
      "10\n",
      "['唉唉你们今天怎么了？平常你们不总说咱爸……', '没有！根本没有！和平，你在背后不也总说咱爸是个明白人吗？', '我那是哄着他玩儿哪！', '妈！爷爷……', '爷爷怎么啦？爷爷有缺点就不许人说呀？……可不嘛，有缺点你说，没缺点你什么呀？不是我批评你们，背后议论人这毛病你们也该改改啦！要不咱爸是明白人，不往心里去，要不然……呦，爸您起来啦，您快坐这儿！', '怎么，又哄着我玩儿呐！', '您瞧这是怎么话儿说的……儿媳年幼无知，说话没有深浅，还请公公海涵。', '好啦好啦，不必下脆。', '谁给您下脆了，我给您盛饭去……唉爸您上了一天班儿也够累的吧？虽然说局里的工作离不开您，可您也得注意身体呀！', '离不开我？离不开我今天怎么把我那办公桌儿给搁到……搁到妇联那屋去啦？']\n",
      "['唉唉你们今天怎么了？平常你们不总说咱爸……', '没有！根本没有！和平，你在背后不也总说咱爸是个明白人吗？', '我那是哄着他玩儿哪！', '妈！爷爷……', '爷爷怎么啦？爷爷有缺点就不许人说呀？……可不嘛，有缺点你说，没缺点你什么呀？不是我批评你们，背后议论人这毛病你们也该改改啦！要不咱爸是明白人，不往心里去，要不然……呦，爸您起来啦，您快坐这儿！', '怎么，又哄着我玩儿呐！', '您瞧这是怎么话儿说的……儿媳年幼无知，说话没有深浅，还请公公海涵。', '好啦好啦，不必下脆。', '谁给您下脆了，我给您盛饭去……唉爸您上了一天班儿也够累的吧？虽然说局里的工作离不开您，可您也得注意身体呀！', '离不开我？离不开我今天怎么把我那办公桌儿给搁到……搁到妇联那屋去啦？']\n",
      "['唉唉你们今天怎么了？平常你们不总说咱爸……', '没有！根本没有！和平，你在背后不也总说咱爸是个明白人吗？', '我那是哄着他玩儿哪！', '妈！爷爷……', '爷爷怎么啦？爷爷有缺点就不许人说呀？……可不嘛，有缺点你说，没缺点你什么呀？不是我批评你们，背后议论人这毛病你们也该改改啦！要不咱爸是明白人，不往心里去，要不然……呦，爸您起来啦，您快坐这儿！', '怎么，又哄着我玩儿呐！', '您瞧这是怎么话儿说的……儿媳年幼无知，说话没有深浅，还请公公海涵。', '好啦好啦，不必下脆。', '谁给您下脆了，我给您盛饭去……唉爸您上了一天班儿也够累的吧？虽然说局里的工作离不开您，可您也得注意身体呀！', '离不开我？离不开我今天怎么把我那办公桌儿给搁到……搁到妇联那屋去啦？']\n",
      "['唉唉你们今天怎么了？平常你们不总说咱爸……', '没有！根本没有！和平，你在背后不也总说咱爸是个明白人吗？', '我那是哄着他玩儿哪！', '妈！爷爷……', '爷爷怎么啦？爷爷有缺点就不许人说呀？……可不嘛，有缺点你说，没缺点你什么呀？不是我批评你们，背后议论人这毛病你们也该改改啦！要不咱爸是明白人，不往心里去，要不然……呦，爸您起来啦，您快坐这儿！', '怎么，又哄着我玩儿呐！', '您瞧这是怎么话儿说的……儿媳年幼无知，说话没有深浅，还请公公海涵。', '好啦好啦，不必下脆。', '谁给您下脆了，我给您盛饭去……唉爸您上了一天班儿也够累的吧？虽然说局里的工作离不开您，可您也得注意身体呀！', '离不开我？离不开我今天怎么把我那办公桌儿给搁到……搁到妇联那屋去啦？']\n",
      "['唉唉你们今天怎么了？平常你们不总说咱爸……', '没有！根本没有！和平，你在背后不也总说咱爸是个明白人吗？', '我那是哄着他玩儿哪！', '妈！爷爷……', '爷爷怎么啦？爷爷有缺点就不许人说呀？……可不嘛，有缺点你说，没缺点你什么呀？不是我批评你们，背后议论人这毛病你们也该改改啦！要不咱爸是明白人，不往心里去，要不然……呦，爸您起来啦，您快坐这儿！', '怎么，又哄着我玩儿呐！', '您瞧这是怎么话儿说的……儿媳年幼无知，说话没有深浅，还请公公海涵。', '好啦好啦，不必下脆。', '谁给您下脆了，我给您盛饭去……唉爸您上了一天班儿也够累的吧？虽然说局里的工作离不开您，可您也得注意身体呀！', '离不开我？离不开我今天怎么把我那办公桌儿给搁到……搁到妇联那屋去啦？']\n",
      "['唉唉你们今天怎么了？平常你们不总说咱爸……', '没有！根本没有！和平，你在背后不也总说咱爸是个明白人吗？', '我那是哄着他玩儿哪！', '妈！爷爷……', '爷爷怎么啦？爷爷有缺点就不许人说呀？……可不嘛，有缺点你说，没缺点你什么呀？不是我批评你们，背后议论人这毛病你们也该改改啦！要不咱爸是明白人，不往心里去，要不然……呦，爸您起来啦，您快坐这儿！', '怎么，又哄着我玩儿呐！', '您瞧这是怎么话儿说的……儿媳年幼无知，说话没有深浅，还请公公海涵。', '好啦好啦，不必下脆。', '谁给您下脆了，我给您盛饭去……唉爸您上了一天班儿也够累的吧？虽然说局里的工作离不开您，可您也得注意身体呀！', '离不开我？离不开我今天怎么把我那办公桌儿给搁到……搁到妇联那屋去啦？']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['唉唉你们今天怎么了？平常你们不总说咱爸……', '没有！根本没有！和平，你在背后不也总说咱爸是个明白人吗？', '我那是哄着他玩儿哪！', '妈！爷爷……', '爷爷怎么啦？爷爷有缺点就不许人说呀？……可不嘛，有缺点你说，没缺点你什么呀？不是我批评你们，背后议论人这毛病你们也该改改啦！要不咱爸是明白人，不往心里去，要不然……呦，爸您起来啦，您快坐这儿！', '怎么，又哄着我玩儿呐！', '您瞧这是怎么话儿说的……儿媳年幼无知，说话没有深浅，还请公公海涵。', '好啦好啦，不必下脆。', '谁给您下脆了，我给您盛饭去……唉爸您上了一天班儿也够累的吧？虽然说局里的工作离不开您，可您也得注意身体呀！', '离不开我？离不开我今天怎么把我那办公桌儿给搁到……搁到妇联那屋去啦？']\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[106], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m vocab_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/lr/h1k/KNN-BERT-main/csi_sequetial_fine_tuning/chinese_roberta_wwm_large_ext_pytorch/vocab.txt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      4\u001b[0m bond_vocab\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m671\u001b[39m,\u001b[38;5;241m7992\u001b[39m]\n\u001b[0;32m----> 5\u001b[0m \u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfrom_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43mto_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbond_vocab\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[94], line 11\u001b[0m, in \u001b[0;36mprocess\u001b[0;34m(path_from, path_to, vocab_file, vocab_bond)\u001b[0m\n\u001b[1;32m      8\u001b[0m vocab\u001b[38;5;241m=\u001b[39m[i\u001b[38;5;241m.\u001b[39mstrip(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m vocab][vocab_bond[\u001b[38;5;241m0\u001b[39m]:vocab_bond[\u001b[38;5;241m1\u001b[39m]]\n\u001b[1;32m     10\u001b[0m name\u001b[38;5;241m=\u001b[39mpath_from\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m data\u001b[38;5;241m=\u001b[39m \u001b[43mcontext_window\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdj\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepisodes\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path_to, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     14\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(data, f, ensure_ascii\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "Cell \u001b[0;32mIn[103], line 30\u001b[0m, in \u001b[0;36mcontext_window\u001b[0;34m(epis, vocab, name)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m pointer \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(scene)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m:\n\u001b[1;32m     29\u001b[0m     window_size\u001b[38;5;241m=\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(norm_window_size\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m,norm_window_size)\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mexamine_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpointer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpointer\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mwindow_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43mscene\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     31\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28mprint\u001b[39m(window_size)\n",
      "Cell \u001b[0;32mIn[104], line 4\u001b[0m, in \u001b[0;36mexamine_length\u001b[0;34m(index_top, index_bottom, scene)\u001b[0m\n\u001b[1;32m      2\u001b[0m ref\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(index_top, index_bottom):\n\u001b[0;32m----> 4\u001b[0m     ref\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[43mscene\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspeakers\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m+\u001b[39mscene[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtranscript\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      5\u001b[0m     window_num \u001b[38;5;241m=\u001b[39m ceil(\u001b[38;5;28mlen\u001b[39m(ref)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m512\u001b[39m) \n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m window_num\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "from_path='./My_home.json'\n",
    "to_path='./CRECIL_train.json'\n",
    "vocab_file='/home/lr/h1k/KNN-BERT-main/csi_sequetial_fine_tuning/chinese_roberta_wwm_large_ext_pytorch/vocab.txt'\n",
    "bond_vocab=[671,7992]\n",
    "process(from_path,to_path,vocab_file,bond_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54574f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
